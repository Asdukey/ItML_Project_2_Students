{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications import ResNet50V2, VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Veggie Classification\n",
    "\n",
    "For this assignment you'll need to classify some images of vegetables. \n",
    "\n",
    "## Parts\n",
    "\n",
    "Please do two separate classifications:\n",
    "<ol>\n",
    "<li> First, create a model from scratch. \n",
    "<li> Use transfer learning to use a pretrained model of your choice, adapted to this data. \n",
    "</ol>\n",
    "\n",
    "There won't be an explicit evaluation of accuracy, but you should take some steps to make each model as accurate as you reasonably can, any tuning option is fair game. Along with that, please structure it into a notebook that is well structured and clear that explains what you did and found. Think about:\n",
    "<ul>\n",
    "<li> Sections and headings. \n",
    "<li> A description of the approach taken (e.g. what did you do to determine size, tune, evaluate, etc...)\n",
    "<li> Visualization of some important things such as a confusion matrix and maybe some images. \n",
    "<li> Results, mainly focused on the scoring of the test data. \n",
    "</ul>\n",
    "\n",
    "The descriptions and explainations should highlight the choices you made and why you made them. Figure up to about a page or so worth of text total, explain what happened but don't write an essay. \n",
    "\n",
    "## Deliverables\n",
    "\n",
    "Please sumbmit a link to your github, where everyhting is fully run with all the outputs showing on the page. As well, in the notebook please add some kind of switch controlled by a variable that will control if the notebook runs to train the model or to load the model in from weights - so I can download it and click run all, it will load the saved weights, and predict.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The code in the start of this notebook will download and unzip the dataset, and there is also a simple example of creating datasets. You can change the dataset bit to use a different approach if you'd like. The data is already split into train, validation, and test sets. Please treat the separate test set as the final test set, and don't use it for any training or validation. Each folder name is its own label.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Marking will be based on the following:\n",
    "<ul>\n",
    "<li> Models are cretaed, tuned, and effective at classifying the data: 40%\n",
    "<li> Descriptions and explanations of the approach taken: 20%\n",
    "<li> Code is well structured and clear: 20%\n",
    "</ul>\n",
    "\n",
    "Overall the marking is pretty simple and direct, walk through the process of predicting the veggies, explain what you did, and show the results. If you do that, it'll get a good mark.\n",
    "\n",
    "### Tips\n",
    "\n",
    "Some hints that may be helpful to keep in mind:\n",
    "<ul>\n",
    "<li> The data is pretty large, so you'll want to use datasets rather than load everything into memory. The Keras docs have a few examples of different ways to load image data, our examples showed image generators and the image from directory datasets.  \n",
    "<li> Be careful of batch size, you may hit the colab limits. \n",
    "<li> You'll want to use checkpoints so you can let it train and pick up where you left off.\n",
    "<li> When developing, using a smaller dataset sample is a good idea. These weights could also be saved and loaded to jump start training on the full data. \n",
    "<li>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Unzip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_custom(current, total, width=80):\n",
    "    print(\"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total))\n",
    "import wget\n",
    "import zipfile\n",
    "zip_name = \"train.zip\"\n",
    "\n",
    "url = \"https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/vegetable_image_dataset.zip\"\n",
    "\n",
    "if not os.path.exists(zip_name):\n",
    "    wget.download(url, zip_name, bar=bar_custom)\n",
    "\n",
    "with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 files belonging to 15 classes.\n",
      "Found 3000 files belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generate Datasets - you can change this if desired\n",
    "# ENSURE FILE PATHS MATCH CORRECTLY\n",
    "IMAGE_SIZE=(224,224)\n",
    "train_dir='Vegetable Images/train'\n",
    "val_dir='Vegetable Images/validation'\n",
    "batch_size = 16\n",
    "\n",
    "# Load training data\n",
    "train_ds = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size = IMAGE_SIZE,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size = IMAGE_SIZE,\n",
    "    batch_size = batch_size,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 images belonging to 15 classes.\n",
      "Found 3000 images belonging to 15 classes.\n",
      "Found 3000 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set the image size and batch size\n",
    "IMAGE_SIZE=(224,224)\n",
    "batch_size = 16\n",
    "\n",
    "# Define the paths to the training and validation directories\n",
    "train_dir='Vegetable Images/train'\n",
    "val_dir='Vegetable Images/validation'\n",
    "test_dir='Vegetable Images/test'\n",
    "\n",
    "# Define the data augmentation for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Define the data augmentation for the validation set\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# Load the training data\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Load the validation data\n",
    "val_ds = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load the test data\n",
    "test_ds = val_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "938/938 [==============================] - 530s 564ms/step - loss: 1.8620 - accuracy: 0.3694 - val_loss: 1.1387 - val_accuracy: 0.5973\n",
      "Epoch 2/20\n",
      "938/938 [==============================] - 444s 473ms/step - loss: 1.2934 - accuracy: 0.5627 - val_loss: 0.8646 - val_accuracy: 0.7393\n",
      "Epoch 3/20\n",
      "938/938 [==============================] - 433s 461ms/step - loss: 1.0053 - accuracy: 0.6685 - val_loss: 0.6824 - val_accuracy: 0.7703\n",
      "Epoch 4/20\n",
      "938/938 [==============================] - 438s 467ms/step - loss: 0.8197 - accuracy: 0.7298 - val_loss: 0.4185 - val_accuracy: 0.8730\n",
      "Epoch 5/20\n",
      "938/938 [==============================] - 444s 473ms/step - loss: 0.7079 - accuracy: 0.7708 - val_loss: 0.3550 - val_accuracy: 0.8943\n",
      "Epoch 6/20\n",
      "938/938 [==============================] - 452s 482ms/step - loss: 0.6360 - accuracy: 0.7977 - val_loss: 0.3025 - val_accuracy: 0.9113\n",
      "Epoch 7/20\n",
      "938/938 [==============================] - 462s 493ms/step - loss: 0.5691 - accuracy: 0.8184 - val_loss: 0.2957 - val_accuracy: 0.9130\n",
      "Epoch 8/20\n",
      "938/938 [==============================] - 458s 489ms/step - loss: 0.5240 - accuracy: 0.8327 - val_loss: 0.2911 - val_accuracy: 0.9157\n",
      "Epoch 9/20\n",
      "938/938 [==============================] - 440s 469ms/step - loss: 0.4775 - accuracy: 0.8506 - val_loss: 0.2603 - val_accuracy: 0.9237\n",
      "Epoch 10/20\n",
      "938/938 [==============================] - 441s 471ms/step - loss: 0.4286 - accuracy: 0.8630 - val_loss: 0.3268 - val_accuracy: 0.9033\n",
      "Epoch 11/20\n",
      "938/938 [==============================] - 468s 499ms/step - loss: 0.4227 - accuracy: 0.8706 - val_loss: 0.1991 - val_accuracy: 0.9447\n",
      "Epoch 12/20\n",
      "938/938 [==============================] - 444s 473ms/step - loss: 0.4101 - accuracy: 0.8713 - val_loss: 0.1793 - val_accuracy: 0.9520\n",
      "Epoch 13/20\n",
      "938/938 [==============================] - 433s 461ms/step - loss: 0.3710 - accuracy: 0.8830 - val_loss: 0.1336 - val_accuracy: 0.9587\n",
      "Epoch 14/20\n",
      "938/938 [==============================] - 432s 461ms/step - loss: 0.3621 - accuracy: 0.8883 - val_loss: 0.1930 - val_accuracy: 0.9467\n",
      "Epoch 15/20\n",
      "938/938 [==============================] - 439s 468ms/step - loss: 0.3427 - accuracy: 0.8941 - val_loss: 0.2056 - val_accuracy: 0.9423\n",
      "Epoch 16/20\n",
      "938/938 [==============================] - 437s 466ms/step - loss: 0.3501 - accuracy: 0.8929 - val_loss: 0.2065 - val_accuracy: 0.9480\n",
      "Epoch 17/20\n",
      "938/938 [==============================] - 431s 460ms/step - loss: 0.3383 - accuracy: 0.8981 - val_loss: 0.1454 - val_accuracy: 0.9597\n",
      "Epoch 18/20\n",
      "938/938 [==============================] - 436s 465ms/step - loss: 0.3145 - accuracy: 0.9047 - val_loss: 0.1177 - val_accuracy: 0.9667\n",
      "Epoch 19/20\n",
      "938/938 [==============================] - 441s 471ms/step - loss: 0.3058 - accuracy: 0.9030 - val_loss: 0.1855 - val_accuracy: 0.9413\n",
      "Epoch 20/20\n",
      "938/938 [==============================] - 436s 465ms/step - loss: 0.2947 - accuracy: 0.9121 - val_loss: 0.0970 - val_accuracy: 0.9690\n",
      "188/188 - 25s - loss: 0.0935 - accuracy: 0.9737 - 25s/epoch - 133ms/step\n",
      "Test accuracy: 0.9736666679382324\n"
     ]
    }
   ],
   "source": [
    "# Define a custom CNN architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(15, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
    "\n",
    "# Print the accuracy of the model on the test set\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 1s 0us/step\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 1132s 1s/step - loss: 0.4838 - accuracy: 0.8550 - val_loss: 0.1261 - val_accuracy: 0.9633\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1129s 1s/step - loss: 0.2604 - accuracy: 0.9286 - val_loss: 0.1854 - val_accuracy: 0.9560\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1152s 1s/step - loss: 0.2346 - accuracy: 0.9394 - val_loss: 0.1148 - val_accuracy: 0.9767\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1131s 1s/step - loss: 0.2059 - accuracy: 0.9507 - val_loss: 0.0854 - val_accuracy: 0.9800\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1134s 1s/step - loss: 0.2089 - accuracy: 0.9536 - val_loss: 0.1091 - val_accuracy: 0.9833\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1126s 1s/step - loss: 0.1973 - accuracy: 0.9589 - val_loss: 0.0516 - val_accuracy: 0.9897\n",
      "Epoch 7/10\n",
      "858/938 [==========================>...] - ETA: 1:18 - loss: 0.1586 - accuracy: 0.9645"
     ]
    }
   ],
   "source": [
    "# Load the VGG16 model without the top layer\n",
    "base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the pre-trained layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add new classification layer\n",
    "flat_layer = tf.keras.layers.Flatten()(base_model.output)\n",
    "class_layer = tf.keras.layers.Dense(15, activation='softmax')(flat_layer)\n",
    "model_transfer = tf.keras.models.Model(inputs=base_model.input, outputs=class_layer)\n",
    "\n",
    "# Compile the model\n",
    "model_transfer.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_transfer = model_transfer.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Best Models and Illustrate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IMAGE_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18508\\701520979.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtest_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlabel_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mimage_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IMAGE_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "test_dir = 'Vegetable Images/test'\n",
    "test_ds = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate the custom model\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=0)\n",
    "print(\"Custom Model - Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the transfer learning model\n",
    "loss, accuracy = model_transfer.evaluate(test_ds, verbose=0)\n",
    "print(\"Transfer Learning Model - Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  can also create a confusion matrix to visualize the performance of the models on each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true labels and predicted labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for images, labels in test_ds:\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(model_transfer.predict(images), axis=1))\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False, square=True)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additionally, we can visualize some images and their predicted labels from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images and their true labels\n",
    "image_batch, label_batch = test_ds.as_numpy_iterator().next()\n",
    "predictions = model_transfer.predict_on_batch(image_batch).flatten()\n",
    "\n",
    "# Convert the true labels and predicted labels to class names\n",
    "class_names = train_ds.class_names\n",
    "true_labels = [class_names[np.argmax(label)] for label in label_batch]\n",
    "predicted_labels = [class_names[np.argmax(prediction)] for prediction in predictions]\n",
    "\n",
    "# Plot the images and predicted labels\n",
    "fig, axs = plt.subplots(4, 4, figsize=(15, 15))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(len(image_batch)):\n",
    "    axs[i].imshow(image_batch[i].astype('uint8'))\n",
    "    axs[i].set_title(f\"True: {true_labels[i]}, Predicted: {predicted_labels[i]}\")\n",
    "    axs[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer.save('model_trans')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
